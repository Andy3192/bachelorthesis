{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86759f51-730c-4e8d-99d6-b63d5cf7f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for ordinary distribution\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "\n",
    "# Import file\n",
    "desktop_path = os.path.expanduser(\"~/Documents/Ausbildung/Studium/DSHS - SGP/Module/Bachelorarbeit/Ergebnisse\")\n",
    "diagram_path = os.path.join(desktop_path, \"Uebersicht.xlsx\")\n",
    "\n",
    "metrics = [\"Tore Poisson Bundesliga\", \n",
    "           \"Tore Poisson La Liga\",\n",
    "           \"Tore Poisson Ligue 1\",\n",
    "           \"Tore Poisson Premier League\",\n",
    "           \"Tore Poisson Serie A\",\n",
    "           \"Schüsse Bundesliga\",\n",
    "           \"Schüsse La Liga\",\n",
    "           \"Schüsse Ligue 1\",\n",
    "           \"Schüsse Premier League\",\n",
    "           \"Schüsse Serie A\",\n",
    "           \"VAEP Bundesliga\",\n",
    "           \"VAEP La Liga\",\n",
    "           \"VAEP Ligue 1\",\n",
    "           \"VAEP Premier League\",\n",
    "           \"VAEP Serie A\",\n",
    "           \"xG Bundesliga\",\n",
    "           \"xG La Liga\",\n",
    "           \"xG Ligue 1\",\n",
    "           \"xG Premier League\",\n",
    "           \"xG Serie A\",\n",
    "           \"xT Bundesliga\",\n",
    "           \"xT La Liga\",\n",
    "           \"xT Ligue 1\",\n",
    "           \"xT Premier League\",\n",
    "           \"xT Serie A\"]\n",
    "\n",
    "# Iterate through each metric and perform the Shapiro-Wilk test\n",
    "for metric in metrics:\n",
    "    # Load the data for the current metric (sheet with the name of the metric)\n",
    "    try:\n",
    "        df = pd.read_excel(diagram_path, sheet_name=metric)\n",
    "        \n",
    "        # Extract the second column (assuming data starts from the second column)\n",
    "        data = df.iloc[:, 1].dropna().values  # Remove NaN values\n",
    "        print(data)\n",
    "        if len(data) >= 3:  # Ensure there are enough data points\n",
    "            # Perform the Shapiro-Wilk test\n",
    "            stat, p = shapiro(data)\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"Shapiro-Wilk Test for {metric}:\")\n",
    "            print(f\"Test statistic: {stat}, p-value: {p}\")\n",
    "            \n",
    "            # Interpret results\n",
    "            if p > 0.05:\n",
    "                print(f\"  - Data for {metric} is normally distributed.\\n\")\n",
    "            else:\n",
    "                print(f\"  - Data for {metric} is not normally distributed.\\n\")\n",
    "        else:\n",
    "            print(f\"Not enough data for {metric}. Skipping Shapiro-Wilk test.\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {metric}: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e819199-2963-4710-9d7c-7a28c9251945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check metrics within leagues with Friedman's Test \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# Import file\n",
    "desktop_path = os.path.expanduser(\"~/Documents/Ausbildung/Studium/DSHS - SGP/Module/Bachelorarbeit/Ergebnisse\")\n",
    "diagram_path = os.path.join(desktop_path, \"Uebersicht.xlsx\")\n",
    "\n",
    "# Load the sheet \"Bundesliga gesamt\"\n",
    "sheet_name = \"Bundesliga Gesamt\"\n",
    "df = pd.read_excel(diagram_path, sheet_name=sheet_name)\n",
    "\n",
    "# Define metric names and corresponding columns\n",
    "metrics = [\"Tore\", \"Schüsse\", \"VAEP\", \"xG\", \"xT\"]\n",
    "columns = [0, 1, 2, 3, 4]  # Column indexes \n",
    "\n",
    "# Extract data, ensuring we start from row 2 (excluding header)\n",
    "data = df.iloc[0:, columns].dropna()  # Drop rows with missing values\n",
    "\n",
    "# Convert to NumPy array\n",
    "data_array = data.to_numpy()\n",
    "\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"First few rows of data:\\n\", data.head())\n",
    "\n",
    "# Perform the Friedman test\n",
    "if len(data) >= 3:  # Friedman test requires at least 3 observations\n",
    "    stat, p = friedmanchisquare(*[data.iloc[:, i].values for i in range(len(metrics))])\n",
    "\n",
    "    print(f\"Friedman Test Results:\")\n",
    "    print(f\"Test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "    if p < 0.05:\n",
    "        print(\"There is a significant difference between the metrics.\")\n",
    "\n",
    "        import scipy.stats as stats\n",
    "        import scikit_posthocs as sp\n",
    "\n",
    "        # Convert data to long format for Dunn’s test\n",
    "        long_data = pd.melt(data.reset_index(), id_vars=[\"index\"], value_vars=metrics, var_name=\"Metric\", value_name=\"Value\")\n",
    "    \n",
    "        # Perform Dunn's test with Bonferroni correction\n",
    "        dunn_results = sp.posthoc_dunn(long_data, val_col=\"Value\", group_col=\"Metric\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        print(\"\\nPairwise Dunn’s Test Results (p-values):\")\n",
    "        print(dunn_results)\n",
    "\n",
    "        # Ranking the metrics (higher rank = better performance)\n",
    "        avg_ranks = data.rank(axis=1, method=\"average\").mean()\n",
    "        best_metric = avg_ranks.idxmax()\n",
    "        \n",
    "        print(\"\\nAverage Ranks of Metrics:\")\n",
    "        print(avg_ranks)\n",
    "        print(f\"\\nThe best metric based on rankings: {best_metric} \")\n",
    "\n",
    "    else:\n",
    "        print(\"No significant difference between the metrics.\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data to perform Friedman and Nemenyi test.\")\n",
    "\n",
    "\n",
    "# Export results\n",
    "desktop_path = os.path.expanduser(\"~/Desktop\")\n",
    "export_path = os.path.join(desktop_path, \"Dunns_Test_Results.xlsx\")\n",
    "\n",
    "# Save results to Excel\n",
    "with pd.ExcelWriter(export_path) as writer:\n",
    "    dunn_results.to_excel(writer, sheet_name=\"Dunns Test P-Values\")  # Export p-values\n",
    "    avg_ranks.to_frame(name=\"Average Ranks\").to_excel(writer, sheet_name=\"Metric Rankings\")  # Export ranking\n",
    "\n",
    "print(f\"Results saved successfully to: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef56c89-983b-4e62-81ea-4ea4b493bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check differences across leagues with Kruskal-Wallis-Test\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# Import file\n",
    "desktop_path = os.path.expanduser(\"~/Documents/Ausbildung/Studium/DSHS - SGP/Module/Bachelorarbeit/Ergebnisse\")\n",
    "diagram_path = os.path.join(desktop_path, \"Uebersicht.xlsx\")\n",
    "\n",
    "# Load the sheet \"Serie A Gesamt\"\n",
    "sheet_name = \"Gesamt gesamt\"\n",
    "df = pd.read_excel(diagram_path, sheet_name=sheet_name)\n",
    "\n",
    "# Define metric names and corresponding columns\n",
    "metrics = [\"Tore\", \"Schüsse\", \"VAEP\", \"xG\", \"xT\"]\n",
    "columns = [0, 1, 2, 3, 4]  # Column indexes \n",
    "\n",
    "# Extract data, ensuring we start from row 2 (excluding header\n",
    "data = df.iloc[0:, columns].dropna()  # Drop rows with missing values\n",
    "\n",
    "# Convert to NumPy array\n",
    "data_array = data.to_numpy()\n",
    "\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"First few rows of data:\\n\", data.head())\n",
    "\n",
    "# Perform the Kruskal-Wallis test\n",
    "if len(data) >= 3:  # Kruskal-Wallis test requires at least 3 observations\n",
    "    # Applying Kruskal-Wallis test across all metrics\n",
    "    stat, p = kruskal(*[data.iloc[:, i].values for i in range(len(metrics))])\n",
    "\n",
    "    print(f\"Kruskal-Wallis Test Results:\")\n",
    "    print(f\"Test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "    if p < 0.05:\n",
    "        print(\"There is a significant difference between the metrics. Running post-hoc Dunn's test...\")\n",
    "\n",
    "        # Convert data to long format for Dunn’s test\n",
    "        long_data = pd.melt(data.reset_index(), id_vars=[\"index\"], value_vars=metrics, var_name=\"Metric\", value_name=\"Value\")\n",
    "    \n",
    "        # Perform Dunn's test with Bonferroni correction\n",
    "        dunn_results = sp.posthoc_dunn(long_data, val_col=\"Value\", group_col=\"Metric\", p_adjust=\"bonferroni\")\n",
    "\n",
    "        print(\"\\nPairwise Dunn’s Test Results (p-values):\")\n",
    "        print(dunn_results)\n",
    "\n",
    "        # Ranking the metrics (higher rank = better performance)\n",
    "        avg_ranks = data.rank(axis=1, method=\"average\").mean()\n",
    "        best_metric = avg_ranks.idxmin()  # Best metric is the one with the lowest rank\n",
    "\n",
    "        print(\"\\nAverage Ranks of Metrics:\")\n",
    "        print(avg_ranks)\n",
    "        print(f\"\\n The best metric based on rankings: {best_metric}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No significant difference between the metrics.\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data to perform Kruskal-Wallis and Dunn's test.\")\n",
    "\n",
    "# Export results\n",
    "desktop_path = os.path.expanduser(\"~/Desktop\")\n",
    "export_path = os.path.join(desktop_path, \"Dunns_Test_Results_Kruskal_Wallis.xlsx\")\n",
    "\n",
    "# Save results to Excel\n",
    "with pd.ExcelWriter(export_path) as writer:\n",
    "    dunn_results.to_excel(writer, sheet_name=\"Dunns Test P-Values\")  # Export p-values\n",
    "    avg_ranks.to_frame(name=\"Average Ranks\").to_excel(writer, sheet_name=\"Metric Rankings\")  # Export ranking\n",
    "\n",
    "print(f\"Results saved successfully to: {export_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edff7f8-8f6a-4cb3-b596-d30559acab8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
